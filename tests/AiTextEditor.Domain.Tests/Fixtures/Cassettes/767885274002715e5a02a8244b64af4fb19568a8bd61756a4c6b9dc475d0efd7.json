{
  "StatusCode": 200,
  "Body": "{\u0022id\u0022:\u0022chatcmpl-890\u0022,\u0022object\u0022:\u0022chat.completion\u0022,\u0022created\u0022:1765450408,\u0022model\u0022:\u0022gpt-oss:120b-cloud\u0022,\u0022system_fingerprint\u0022:\u0022fp_ollama\u0022,\u0022choices\u0022:[{\u0022index\u0022:0,\u0022message\u0022:{\u0022role\u0022:\u0022assistant\u0022,\u0022content\u0022:\u0022{\\\u0022status\\\u0022:\\\u0022continue\\\u0022,\\\u0022result\\\u0022:\\\u0022\\\u0022}\u0022,\u0022reasoning\u0022:\u0022We need to continue scanning backward until we find a paragraph containing word \\\u0022\u041F\u043E\u043D\u0447\u0438\u043A\\\u0022. So we must keep requesting next batch? The user gave many chunks already but none contain \\\u0022\u041F\u043E\u043D\u0447\u0438\u043A\\\u0022. We need to continue until find it; we have to request next portion. So respond with status \\\u0022continue\\\u0022. Provide no result.\u0022},\u0022finish_reason\u0022:\u0022stop\u0022}],\u0022usage\u0022:{\u0022prompt_tokens\u0022:13111,\u0022completion_tokens\u0022:85,\u0022total_tokens\u0022:13196}}\n",
  "ContentType": "application/json",
  "Headers": {
    "Date": [
      "Thu, 11 Dec 2025 10:53:28 GMT"
    ],
    "Transfer-Encoding": [
      "chunked"
    ]
  }
}